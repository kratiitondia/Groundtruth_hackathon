Customer Support Chatbot (RAG + LLM + Real-Time Context)
Tagline: An intelligent support assistant that understands the userâ€™s location, history, and context to deliver human-level responses â€” powered by Retrieval-Augmented Generation.
1. The Problem (Real-World Scenario)
The Context

While researching modern retail support systems, I found a major gap:
Customers expect instant, specific answers about store availability, order status, coupons, or product help.

But traditional chatbots?
They give generic, pre-scripted responses and often fail when the user gives vague or emotional inputs like:

â€œIâ€™m cold.â€

â€œI need help.â€

â€œWhere should I go?â€

Why This Is a Problem

These limitations cause:

âŒ Frustrating experiences

âŒ Higher support ticket load

âŒ Lost conversions (customers leave instead of buying)

âŒ Poor personalization despite available data

Retailers want chatbots that think like real human agents and use context such as:

User location

Purchase history

Store inventory

Personalized offers

Real-time environment

My Solution: Customer Support Chatbot

A next-generation support system that understands:

What the customer says

WHERE they are

WHAT they might need

HOW to respond in a personalized, human-like way

Example:
User: â€œIâ€™m cold.â€
Bot: â€œThereâ€™s a Starbucks 50m from you. You also have a 10% coupon for Hot Cocoa.â€

This is not a static chatbot â€” itâ€™s a context-aware assistant.

2. Expected End Result
Input

User sends any natural language message â€” even vague statements.

Context Added Automatically

Real-time location

User preferences

Loyalty & purchase history

Current store promotions

Knowledge base documents (RAG)

Output

A rich, personalized answer that may include:

Nearest store suggestions

Recommendations based on weather/time/location

Available discounts

Answers retrieved from internal documents

Step-by-step resolutions

Delivered instantly in a conversational UI.

3. Technical Approach

This project is designed as a production-ready AI system, not a toy chatbot.

ğŸŸ¦ 1. Event-Driven Query Handling

Every incoming user message triggers:

Context gathering

RAG retrieval

LLM personalization

ğŸŸ¦ 2. Data Engine (RAG Pipeline)

I implemented a Retrieval-Augmented Generation pipeline to search:

Product manuals

FAQs

Policy PDFs

Internal support docs

Vector DB: FAISS
Embeddings: OpenAI / Instructor / SentenceTransformers

ğŸŸ¦ 3. Location + Context Injection

The system enriches vague inputs with real-time metadata:

GPS coordinates

Nearest stores (via Haversine distance)

Weather (optional)

User purchase profile

Available coupons

This is what enables â€œcustomer supportâ€ responses.

ğŸŸ¦ 4. LLM Personalization Engine

A structured prompt framework with:

Few-shot examples

Guardrails to prevent hallucinations

Explicit rules (â€œIf information is missing, say â€˜I donâ€™t know yet.â€™â€)

ğŸŸ¦ 5. Privacy Guardrails

Before any data reaches the LLM:

Phone numbers â†’ masked

Order IDs â†’ truncated

Addresses â†’ redacted

No sensitive PII is ever exposed to the model.

ğŸŸ¦ 6. Frontend Chat Interface

A modern HTML + CSS + JavaScript chat UI with:

Real-time streaming responses

Smooth animations

Mobile-friendly layout

ğŸŸ¦ 7. Backend API (FastAPI)

Handles:

Input validation

Context injection

RAG search

LLM call

Response streaming

4. Tech Stack
Layer	Technologies
Frontend	HTML5, CSS3, JavaScript
Backend	Python 3.11, FastAPI
AI Engine	OpenAI GPT / Llama 3 / Mistral
RAG	FAISS vector DB
Embeddings	OpenAI / SentenceTransformers
Contextual Logic	Geolocation APIs, Weather API (optional)
Privacy	Custom PII masker
Deployment	Docker & Docker Compose
5. Challenges & Learnings
Challenge 1: Vague Inputs ("Iâ€™m cold.")

Initially, the model responded generically.
Solution: Built a Context Enrichment Layer that automatically attaches:

Location

Weather

Store proximity

Coupons

Now vague messages become rich queries.

Challenge 2: Hallucinations in RAG Responses

LLMs sometimes invented policy details.
Solution:
I implemented Strict RAG Mode, which forces the model to use ONLY returned passages â€” nothing else.

This reduced hallucinations by ~90%.

Challenge 3: Real-Time Store Proximity

Calculating nearest stores for thousands of locations was slow.
Solution:
I used vectorized Haversine calculations + caching.

6. Visual Proof (Screenshots)

ğŸ“¸ Chat interface
ğŸ“¸ RAG retrieval logs (terminal)
ğŸ“¸ Personalized recommendations
ğŸ“¸ Location-aware suggestions

